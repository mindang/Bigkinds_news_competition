{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이트닝 참고\n",
    "\n",
    "https://baeseongsu.github.io/posts/pytorch-lightning-introduction/ - train/val step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split, DataLoader , Dataset , TensorDataset\n",
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import BertModel\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RM_bert(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 하이퍼 파라미터\n",
    "        self.num_labels=1\n",
    "        self.hidden_size=768\n",
    "        self.hidden_dropout_prob=0.1\n",
    "\n",
    "        # 모델 구조\n",
    "        self.kobert = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "        self.dropout = nn.Dropout(self.hidden_dropout_prob)\n",
    "        self.linear = nn.Linear(self.hidden_size,  self.num_labels)\n",
    "\n",
    "        # 최적화 파라미터\n",
    "        #self.ratio_pn = 1 # T/F 비율 얼마나 줄지 ex) T 10개 F 30개면 pos_weight = 3\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()   # sigmoid+BCE\n",
    "\n",
    "    def forward(\n",
    "            self,input_ids=None,\n",
    "            attention_mask=None,\n",
    "            token_type_ids=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            labels=None,\n",
    "            ):\n",
    "        \n",
    "        # KoBERT 입력\n",
    "        output = self.kobert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "        \n",
    "        # 드롭아웃\n",
    "        pooled_output = self.dropout(output.pooler_output)\n",
    "\n",
    "        # 리니어 레이어\n",
    "        return self.linear(pooled_output)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(),lr=0.02)\n",
    "    \n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x1,x2,x3,y = train_batch['x1'] , train_batch['x2'] , train_batch['x3'] , train_batch['y']\n",
    "        logits = self.forward(x1,x2,x3)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        self.log_dict({'train_loss':loss})\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x1,x2,x3,y = val_batch['x1'] , val_batch['x2'] , val_batch['x3'] , val_batch['y']\n",
    "        logits = self.forward(x1,x2,x3)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        self.log_dict({'val_loss':loss})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class News_Dataset(Dataset):\n",
    "    def __init__(self,root_path):\n",
    "        true_path = os.path.join(root_path,str(1))\n",
    "        false_path = os.path.join(root_path,str(0))\n",
    "        true_file_list = [os.path.join(true_path,f) for f in os.listdir(true_path)]\n",
    "        false_file_list = [os.path.join(false_path,f) for f in os.listdir(false_path)]\n",
    "\n",
    "        # 최종 파일경로 모음 및 라벨\n",
    "        self.file_path = true_file_list + false_file_list\n",
    "        self.label = [1 for _ in range(len(true_file_list))] + [0 for _ in range(len(false_file_list))]\n",
    "\n",
    "        # 섞어주기\n",
    "        data = list(zip(self.file_path, self.label))\n",
    "        random.shuffle(data)\n",
    "        self.file_path, self.label = zip(*data)\n",
    "\n",
    "        # 토크나이저\n",
    "        self.tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with open(self.file_path[idx], 'r') as file:\n",
    "            title = file.readline()\n",
    "            content = file.readline()\n",
    "        token = self.tokenizer(title[:-2],content)\n",
    "        y = self.label[idx]\n",
    "        \n",
    "        return {\n",
    "            'x1':torch.tensor(token['input_ids']),\n",
    "            'x2':torch.tensor(token['token_type_ids']),\n",
    "            'x3':torch.tensor(token['attention_mask']),\n",
    "            'y':torch.tensor(y)}\n",
    "    \n",
    "def my_collate_fn(samples):\n",
    "    '''\n",
    "        [{'x1': tensor([0, 1, 2, 3, 4]), 'x2': tensor([0, 0, 0, 0, 0]), 'x3': tensor([1, 1, 1, 1, 1]), 'y': tensor([0, 1, 0, 1, 0])}, \n",
    "        {'x1': tensor([0, 1, 2, 3, 4]), 'x2': tensor([0, 0, 0, 0, 0]), 'x3': tensor([1, 1, 1, 1, 1]), 'y': tensor([0, 1, 0, 1, 0])}]\n",
    "    \n",
    "    '''\n",
    "\n",
    "    collate_x1 = []\n",
    "    collate_x2 = []\n",
    "    collate_x3 = []\n",
    "    collate_y = []\n",
    "    \n",
    "    for sample in samples:\n",
    "        collate_x1.append(sample['x1'])\n",
    "        collate_x2.append(sample['x2'])\n",
    "        collate_x3.append(sample['x3'])\n",
    "        collate_y.append(sample['y'])\n",
    "\n",
    "    \n",
    "    return {'x1': torch.stack(collate_x1),\n",
    "            'x2' : torch.stack(collate_x2),\n",
    "            'x3' : torch.stack(collate_x3),\n",
    "            'y': torch.stack(collate_y)}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = News_Dataset()\n",
    "valid_dataset = News_Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type              | Params\n",
      "----------------------------------------------\n",
      "0 | kobert  | BertModel         | 92.2 M\n",
      "1 | dropout | Dropout           | 0     \n",
      "2 | linear  | Linear            | 769   \n",
      "3 | loss_fn | BCEWithLogitsLoss | 0     \n",
      "----------------------------------------------\n",
      "92.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "92.2 M    Total params\n",
      "368.751   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed5aa1438ec49a3aafc912ad9b8bdcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\miniconda3\\envs\\dl_study\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\Lenovo\\miniconda3\\envs\\dl_study\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\Lenovo\\miniconda3\\envs\\dl_study\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\Lenovo\\miniconda3\\envs\\dl_study\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a5cbb792c249b4b9f1358dce787ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e35a2121a345059d339d9af7e32509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea92f282788c49eaa4a1898caa1196d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b4b0cf1fa9d4786a9355ffa87ec49a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "train_loader =  DataLoader(train_dataset,batch_size=batch_size,shuffle=True,collate_fn=my_collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset,batch_size=batch_size,shuffle=True,collate_fn=my_collate_fn)\n",
    "\n",
    "logger = pl.loggers.CSVLogger(\"logs\", name=\"RM_training1\")\n",
    "trainer = pl.Trainer(max_epochs=3,logger=logger,accelerator=\"auto\")\n",
    "model = RM_bert()\n",
    "trainer.fit(model,train_loader,valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문제\n",
    "\n",
    "0. 크롤링 기사 전처리 , 제목 , 본문 어떻게 할건지? , 스페셜 토큰\n",
    "\n",
    "1. 다이나믹 패딩 collate에 적용\n",
    "\n",
    "2. 얼마나 저장할지 에퐄마다?\n",
    "\n",
    "3. 조기종료\n",
    "\n",
    "4. 스케줄러? 안해도 될듯? 몰루\n",
    "\n",
    "5. py파일로 변환 , arg 밑 도커 백그라운드\n",
    "\n",
    "6. train_loss는 왜 저장안되는지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000],\n",
      "        [1.0000]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loader = DataLoader(News_Dataset(),batch_size=10,shuffle=True,collate_fn=my_collate_fn)\n",
    "sample = next(iter(test_loader))\n",
    "x1,x2,x3,y = sample['x1'] , sample['x2'] , sample['x3'] , sample['y']\n",
    "y_pred = nn.Sigmoid()(model(x1,x2,x3))\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
